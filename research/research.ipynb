{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# purpose of accelerate: \n",
    "1. Ease of Multi-Device Training: Whether you're using multiple GPUs or TPUs, accelerate makes it easier to scale your model accross devices with minimal code\n",
    "2. Mixed percision: it allows models to be trained using mixed precision, which can speed up training and reduce memory useage\n",
    "3. Zero Redundancy Optimizer (ZeRO): Helps manage large models be efficiently splitting the model across multiple devices. Offiad to CPU/SSD: Useful for large models that may not fit entirely into GPU memory, by allowing parts of the model or optimizer to be offloaded tp CPU or even SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (2.6.0+cu126)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (4.51.0)\n",
      "Requirement already satisfied: accelerate in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from accelerate) (2.6.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\pythonprojects\\agentic_ai\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade accelerate\n",
    "! pip install -y transformer accelerate\n",
    "! pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\26amr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\26amr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt \n",
    "from datasets import load_dataset\n",
    "import pandas as pd \n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "# AutoTokenizer - convert text to tokens\n",
    "# utoModelForSeq2SeqLM - load the specific model from hugging face\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amruth', 'the', 'new', 'AI', 'king', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to check tokenizer \n",
    "sentense='Amruth the new AI king!'\n",
    "tokens =word_tokenize(sentense)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Functionality of Hugging Face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"California's largest electricity provider has turned off power to hundreds of thousands of customers.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")\n",
    "inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "# decode the summary back to text \n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model='google/pegasus-cnn_dailymail'\n",
    "# tokenizer = convert text to tokens\n",
    "tokenizer=AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# loading the model and tokenizer \n",
    "model_pegasus=AutoModelForSeq2SeqLM.from_pretrained(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download & unzip data\n",
    "import os \n",
    "import zipfile\n",
    "os.chdir('../')\n",
    "def extract_unzip_file():\n",
    "    unzip_path='dataset'\n",
    "    os.makedirs(unzip_path,exist_ok=True)\n",
    "    with zipfile.ZipFile('dataset/summarizer-data.zip','r') as zip_ref:\n",
    "        zip_ref.extractall(unzip_path)\n",
    "\n",
    "extract_unzip_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the arrow files and convert it into data dictionary \n",
    "dataset_samsun=load_from_disk('dataset/samsum_dataset')\n",
    "dataset_samsun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Lengths(Train/test/validation): [14732, 819, 818]\n",
      "Dialogue:\n",
      "Lenny: Babe, can you help me with something?\n",
      "Bob: Sure, what's up?\n",
      "Lenny: Which one should I pick?\n",
      "Bob: Send me photos\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Bob: I like the first ones best\n",
      "Lenny: But I already have purple trousers. Does it make sense to have two pairs?\n",
      "Bob: I have four black pairs :D :D\n",
      "Lenny: yeah, but shouldn't I pick a different color?\n",
      "Bob: what matters is what you'll give you the most outfit options\n",
      "Lenny: So I guess I'll buy the first or the third pair then\n",
      "Bob: Pick the best quality then\n",
      "Lenny: ur right, thx\n",
      "Bob: no prob :)\n",
      "Summary:\n",
      "Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n"
     ]
    }
   ],
   "source": [
    "split_lengths=[len(dataset_samsun[split]) for split in dataset_samsun]\n",
    "print(f'Split Lengths(Train/test/validation): {split_lengths}')\n",
    "\n",
    "print(f'Dialogue:')\n",
    "print(dataset_samsun['test'][2]['dialogue'])\n",
    "print(f'Summary:')\n",
    "print(dataset_samsun['test'][2]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare the data for training sequence to sequence model\n",
    "- the dialogue and summary must br converted into 3 main fields i.e. input_ids, attention_mask, labels \n",
    "example \n",
    "{'dialogue: 'Hi! How are you? ,\n",
    "'summary: 'The speaker is asking how the other person is.'\n",
    "}\n",
    "\n",
    "{   \n",
    "    'input_ids: [123,453,234,....], # Token IDs for the dialogue \n",
    "\n",
    "    'attention_mask:[1,1,1,....],# Attention mask for special characters to tokens \n",
    "\n",
    "    'labels: [321,654, 987, ...], # Token IDs for the summary (target)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Examples_to_features(example_batch):\n",
    "    input_encodings=tokenizer(example_batch['dialogue'],max_length=1024, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings=tokenizer(example_batch['summary'],max_length=128,truncation=True)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels':target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a675699b97147e991e51d55045461a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonProjects\\Agentic_AI\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56bb287c3c1481c9a9a4bfeece70758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d92cc9660d48e0bbb6ebb65d8a6fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_samsum_pt=dataset_samsun.map(convert_Examples_to_features,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triaining \n",
    "from transformers import DataCollatorForSeq2Seq # it can be provided to model that helps in preparing batch to batches for training   \n",
    "\n",
    "seq2seq_data_collator=DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args=TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_eval_batch_size=1, per_device_train_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10,\n",
    "    eval_steps=500, save_steps=1e6,\n",
    "    eval_strategy='steps',\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26amr\\AppData\\Local\\Temp\\ipykernel_40084\\33411816.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model_pegasus, args=trainer_args,\n"
     ]
    }
   ],
   "source": [
    "trainer=Trainer(model=model_pegasus, args=trainer_args,\n",
    "                tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                train_dataset=dataset_samsum_pt['test'],\n",
    "                eval_dataset=dataset_samsum_pt['validation']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 47:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonProjects\\Agentic_AI\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51, training_loss=3.1008997945224537, metrics={'train_runtime': 2866.5425, 'train_samples_per_second': 0.286, 'train_steps_per_second': 0.018, 'total_flos': 313450454089728.0, 'train_loss': 3.1008997945224537, 'epoch': 0.9963369963369964})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_seized_chunks(list_of_elements, batch_size):\n",
    "    for i in range(0,len(list_of_elements),batch_size):\n",
    "        yield list_of_elements[i:i+batch_size]\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text='article',column_summary='highlights'):\n",
    "    article_batches=list(generate_batch_seized_chunks(dataset[column_text],batch_size))\n",
    "    target_batches=list(generate_batch_seized_chunks(dataset[column_summary],batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches,target_batches), total=len(article_batches)):\n",
    "        inputs=tokenizer(article_batch,max_length=1024, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        summaries=model.generate(input_ids=inputs['input_ids'].to(device), \n",
    "                                 attention_mask=inputs['attention_mask'].to(device),\n",
    "                                 length_penalty=0.8,num_beams=8,max_length=128\n",
    "                                 )\n",
    "        # decode the generated texts \n",
    "        # replace the token, and add the decoded texts with the reference to the metrics\n",
    "        decode_summaries=[\n",
    "            tokenizer.decode(s,skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries\n",
    "        ]\n",
    "        decode_summaries=[d.replace]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
